{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "Keras = tf.keras\n",
    "make_sampling_table = Keras.preprocessing.sequence.make_sampling_table\n",
    "skipgrams = Keras.preprocessing.sequence.skipgrams\n",
    "Model = Keras.models.Model\n",
    "Dense, Dot = Keras.layers.Dense, Keras.layers.dot\n",
    "Embedding, Reshape, Input = Keras.layers.Embedding, Keras.layers.Reshape, Keras.layers.Input\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_dir = '/Users/jujohnson/git/Hcpcs2Vec/'\n",
    "data_dir = os.environ['CMS_RAW']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Using a sample of 20K Medicare Part B records from 2012."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 500000 entries, 7311433 to 5246454\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count   Dtype  \n",
      "---  ------    --------------   -----  \n",
      " 0   npi       500000 non-null  int64  \n",
      " 1   hcpcs     500000 non-null  object \n",
      " 2   count     500000 non-null  float64\n",
      " 3   hcpcs_id  500000 non-null  int64  \n",
      "dtypes: float64(1), int64(2), object(1)\n",
      "memory usage: 19.1+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = os.path.join(\n",
    "    data_dir, \n",
    "    '2012', \n",
    "    'Medicare_Provider_Utilization_and_Payment_Data__Physician_and_Other_Supplier_CY2012.csv.gz')\n",
    "\n",
    "\n",
    "# we only need the NPI and HCPCS Columns\n",
    "columns = {\n",
    "    'National Provider Identifier': 'npi',\n",
    "    'HCPCS Code': 'hcpcs',\n",
    "    'Number of Services': 'count',\n",
    "}\n",
    "\n",
    "data = pd.read_csv(data_file, usecols=list(columns.keys()))\n",
    "data.rename(columns=columns, inplace=True)\n",
    "data = data.sample(500000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create HCPCS <--> ID Mapping\n",
    "\n",
    "Generatea a unique identifier for each HCPCS code.\n",
    "\n",
    "Saves the encoder for future mapping of codes <--> IDS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved HCPCS label encoded classes to /Users/jujohnson/git/Hcpcs2Vec/data/hcpcs-labelencoding.pickle\n",
      "CPU times: user 168 ms, sys: 13 ms, total: 181 ms\n",
      "Wall time: 181 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "data['hcpcs_id'] = le.fit_transform(data['hcpcs'])\n",
    "\n",
    "# save label encoder results to enable inverse transform later\n",
    "hcpcsIdFile = os.path.join(proj_dir, 'data', 'hcpcs-labelencoding.pickle')\n",
    "with open(hcpcsIdFile, 'wb') as fout:\n",
    "    pickle.dump(le.classes_, fout)\n",
    "\n",
    "print(f'Saved HCPCS label encoded classes to {hcpcsIdFile}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract HCPCS Contexts\n",
    "\n",
    "Creates a corpus of HCPCS contexts, or sets.\n",
    "\n",
    "Each set is a group of HCPCS procedure codes that occur in the same context.\n",
    "\n",
    "This context is defined by the procedures performed by a doctor over a given year.\n",
    "\n",
    "We sort each context by the frequency of HCPCS occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jujohnson/git/Hcpcs2Vec/env/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 57s, sys: 684 ms, total: 2min 58s\n",
      "Wall time: 2min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "corpus = []\n",
    "\n",
    "for npi, group in data.groupby(by='npi'):\n",
    "    group.sort_values(by='count', inplace=True)\n",
    "    hcpcs_set = np.asarray(group['hcpcs_id'], dtype='int16')\n",
    "    corpus.append(hcpcs_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length: 309026\n"
     ]
    }
   ],
   "source": [
    "print(f'Corpus length: {len(corpus)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Long Contexts\n",
    "\n",
    "We can reduce the longest sequence from 600+ to 50 by removing the largets 2% of HCPCS sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length: 304555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jujohnson/git/Hcpcs2Vec/env/lib/python3.7/site-packages/ipykernel_launcher.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "lengths = np.array(list(map(lambda x: len(x), corpus)))\n",
    "max_seq_length = np.quantile(lengths, 0.98)\n",
    "corpus = np.array(list(filter(lambda x: len(x) <= max_seq_length, corpus)))\n",
    "print(f'Corpus length: {len(corpus)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skipgram Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = data['hcpcs_id'].nunique()\n",
    "window_size = 5\n",
    "embedding_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50000\n",
      "100000\n",
      "150000\n",
      "200000\n",
      "250000\n",
      "300000\n",
      "CPU times: user 6.94 s, sys: 108 ms, total: 7.05 s\n",
      "Wall time: 7.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "counter = 0\n",
    "\n",
    "sampling_table = make_sampling_table(vocab_size)\n",
    "\n",
    "x, y = [], []\n",
    "\n",
    "for seq in corpus:\n",
    "  couples, labels = skipgrams(seq, vocab_size, window_size=window_size, sampling_table=sampling_table)\n",
    "  x.extend(couples)\n",
    "  y.extend(labels)\n",
    "  if counter % 50000 == 0:\n",
    "    print(counter)\n",
    "  counter += 1\n",
    "\n",
    "x = np.array(x, dtype='int16')\n",
    "word_target, word_context = x[:, 0], x[:, 1]\n",
    "y = np.array(y, dtype='int8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skipgram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define input and embedding layers\n",
    "input_target = Input((1,))\n",
    "input_context = Input((1,))\n",
    "embedding = Embedding(vocab_size, embedding_size, input_length=1, name='embedding')\n",
    "\n",
    "# embed target and context\n",
    "target = embedding(input_target)\n",
    "context = embedding(input_context)\n",
    "\n",
    "# get similarity of two embeddings via dot product\n",
    "dot_product = Dot([target, context], axes=1)\n",
    "\n",
    "# add the sigmoid output layer\n",
    "output = Dense(1, activation='sigmoid')(dot_product)\n",
    "\n",
    "model = Keras.Model(inputs=[input_target, input_context], outputs=output)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3312/3312 [==============================] - 111s 34ms/step - loss: 0.3824\n",
      "Epoch 2/10\n",
      "3312/3312 [==============================] - 106s 32ms/step - loss: 0.3281\n",
      "Epoch 3/10\n",
      "3312/3312 [==============================] - 104s 31ms/step - loss: 0.3202\n",
      "Epoch 4/10\n",
      "3312/3312 [==============================] - 102s 31ms/step - loss: 0.3163\n",
      "Epoch 5/10\n",
      "3312/3312 [==============================] - 111s 33ms/step - loss: 0.3137\n",
      "Epoch 6/10\n",
      "3312/3312 [==============================] - 107s 32ms/step - loss: 0.3120\n",
      "Epoch 7/10\n",
      "3312/3312 [==============================] - 106s 32ms/step - loss: 0.3105\n",
      "Epoch 8/10\n",
      "3312/3312 [==============================] - 108s 33ms/step - loss: 0.3090\n",
      "Epoch 9/10\n",
      "3312/3312 [==============================] - 108s 33ms/step - loss: 0.3079\n",
      "Epoch 10/10\n",
      "3312/3312 [==============================] - 107s 32ms/step - loss: 0.3071\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "  x=[word_target, word_context], y=y,\n",
    "  epochs=10, batch_size=128,\n",
    "  verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hcpcs",
   "language": "python",
   "name": "hcpcs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
